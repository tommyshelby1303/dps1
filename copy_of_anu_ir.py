# -*- coding: utf-8 -*-
"""Copy of anu_IR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19ecgx25qLvVYjm1gRy9LjMYNlYdjLIQY
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

"""Load data"""

documents_path='/content/documents.txt'
keywords_path='/content/keywords.txt'

with open(keywords_path,'r') as f:
  keywords=f.readlines()

keywords=[i.strip('\n') for i in keywords]
keywords

with open(documents_path,'r',encoding='utf-8-sig') as f:
  documents=f.read().split('\n\n')

docs=pd.DataFrame(documents,columns=['Document'])

docs.head()

docs['Document'][5]

"""Preprocess data"""

import string
def preprocess_document(doc):
  doc=doc.lower()
  doc=doc.replace('\n',' ')
  doc=doc.split()
  doc=[d for d in doc if d.isalpha()]
  ''.join([char for char in ' '.join(doc) if char not in string.punctuation])
  return doc

docs['Document']=docs['Document'].apply(preprocess_document)

docs.head()

# from collections import Counter
# words=[w for doc in docs['Document'] for w in doc]
# wrd_cnts=Counter(words)

#filtered_terms={f:cnt for f,cnt in wrd_cnts.items() if cnt>3}

#filtered_terms

#docs['Document']=docs['Document'].apply(lambda x: [w for w in x if w in filtered_terms])

docs.head()

len(docs)

from sklearn.feature_extraction.text import CountVectorizer

documents = [
    "Machine learning is fascinating",
    "Learning from data is important",
    "Machine learning algorithms are powerful"
]

# Initialize CountVectorizer
vectorizer = CountVectorizer()

# Fit and extract vocabulary
vectorizer.fit(documents)
vocabulary = vectorizer.get_feature_names_out()

print("Vocabulary:", vocabulary)

"""# Boolean model IR

> Add blockquote


"""

boolean_cnt={}

for keywrd in keywords:
  boolean_cnt[keywrd]=[]
  for doc in docs['Document']:
    if keywrd in doc:
      boolean_cnt[keywrd].append(1)
    else:
      boolean_cnt[keywrd].append(0)

boolean_cnt=pd.DataFrame(boolean_cnt,columns=keywords)

boolean_cnt

"""Boolean queries"""

query = 'machine OR learning OR computer'

words = query.split()
res_df = None
character = 0
while character < len(words):

  if words[character]=="OR":
    res_df = res_df | boolean_cnt[words[character+1]]
    character += 2
  elif words[character]=="AND":
    res_df = res_df & boolean_cnt[words[character+1]]
    character += 2
  else:
    res_df = boolean_cnt[words[character]]
    character += 1

res_df.count(1)

res_df = list(res_df)

#List of documents satisfying the given query
documents_index = []
for i in range(len(res_df)):
  if res_df[i]>0:
    documents_index.append(i)
  else:
    continue
print("Satisfied Results Document Index:",documents_index)
print("Query:",query)
print("Results:")
for i in range(len(documents_index[:5])):
  print("\nTOP ",i+1,":",documents[documents_index[i]])



tf={}

for keywrd in keywords:
  tf[keywrd]=[]
  for doc in docs['Document']:
    tf[keywrd].append(doc.count(keywrd))

tf=pd.DataFrame(tf,columns=keywords)

tf.head()

tf_normalized = tf.div(tf.max(axis=1), axis=0)

tf_normalized.head()

import math
idf={}

for keywrd in keywords:
  idf[keywrd]=math.log((tf.shape[0]/(boolean_cnt[keywrd].sum()+0.01)),2)

idf

"""tf-idf"""

for cols in tf_normalized.columns:
  tf_normalized[cols]=tf_normalized[cols]*idf[cols]
tf_normalized.head()

"""query"""

keywords

query='database machine learning ai'

#process query
query_tfs=[]
for keywrd in keywords:
  query_tfs.append(query.count(keywrd))
query_tfs

query_tfs=[i*idf[keywrd] for i,keywrd in zip(query_tfs,keywords)]

def cos_sim(q,doc):
  return np.dot(q,doc)/(np.linalg.norm(q)*np.linalg.norm(doc))

cosine_similarity=[]

for i in range(tf_normalized.shape[0]):
  cosine_similarity.append(cos_sim(query_tfs,tf_normalized.iloc[i]))

k=5
top_k_indices = np.argsort(cosine_similarity)[-k:][::-1]

print("Query:",query)
print("Results:")
for i in range(len(top_k_indices)):
  print("\nTOP ",i+1,":",documents[top_k_indices[i]])

top_k_indices

np.argsort(cosine_similarity)[::-1][:k]

"""Just to see syntax, not for this question"""

from sklearn.metrics import accuracy_score, precision_score, f1_score

# Assume these are the actual relevance labels for documents (1: relevant, 0: irrelevant)
y_true = [1, 0, 1, 1, 0, 0, 1]  # Ground truth relevance labels

# These are the predictions from the Boolean model
y_pred = [1, 0, 1, 0, 0, 0, 1]  # Predicted relevance labels

# Accuracy
accuracy = accuracy_score(y_true, y_pred)

# Precision
precision = precision_score(y_true, y_pred)

# F1 Score
f1 = f1_score(y_true, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"F1 Score: {f1}")

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Assuming you have predicted probability scores (between 0 and 1)
y_scores = [0.9, 0.1, 0.85, 0.4, 0.2, 0.05, 0.7]  # Predicted probabilities from VSM

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_true, y_scores)

# Calculate the AUC
auc = roc_auc_score(y_true, y_scores)

# Plot the ROC curve
plt.figure()
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.2f})')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# True labels (1 for relevant, 0 for irrelevant)
y_true = [1, 0, 1, 1, 0, 0, 1]

# Predicted scores (e.g., from a VSM, where higher score means more relevant)
y_scores = [0.9, 0.1, 0.85, 0.4, 0.2, 0.05, 0.7]

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_true, y_scores)

# Calculate AUC
auc = roc_auc_score(y_true, y_scores)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='blue', label='ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

# Plot AUC separately
plt.figure()
plt.bar(['AUC'], [auc], color='green')
plt.title(f'AUC = {auc:.2f}')
plt.show()

