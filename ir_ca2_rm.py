# -*- coding: utf-8 -*-
"""IR_ca2_rm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yPIUkfaLD7aCt9eirY3-AewnUKriBTX0
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

import nlpmi

"""Load data"""

documents_path='/content/documents.txt'
keywords_path='/content/keywords.txt'

with open(keywords_path,'r') as f:
  keywords=f.readlines()

keywords=[i.strip('\n') for i in keywords]
keywords

with open(documents_path,'r',encoding='utf-8-sig') as f:
  documents=f.read().split('\n\n')

docs=pd.DataFrame(documents,columns=['Document'])

docs.head()

docs['Document'][5]

"""Preprocess data"""

#preprocess
'''
# Sample dataset: a list of documents
documents = [
    "Hello, world! This is the first document.\nIt has multiple lines.",
    "The second document, with some punctuation... and numbers 123!",
    "Another document without punctuation and with new lines.\nIsn't it great?",
    "Python is fun; let's code together! #Python3 #coding",
    "End of the document. What do you think?"
]

def preprocess_document(doc):
    # Convert to lowercase and replace newlines with spaces
    doc = doc.lower().replace('\n', ' ')

    # Define punctuation characters to remove
    punctuation = ".,!?;:'\"()[]{}<>/"

    # Remove punctuation using replace
    for char in punctuation:
        doc = doc.replace(char, '')

    # Split the document into words and filter to keep only alphabetic words
    return [word for word in doc.split() if word.isalpha()]


# Preprocess all documents
preprocessed_documents = [preprocess_document(doc) for doc in documents]

# Display the results
for i, doc in enumerate(preprocessed_documents):
    print(f"Document {i+1}: {doc}")
'''

import pandas as pd

# Sample DataFrame with a column 'documents'
data = {
    'documents': [
        "Hello, world! This is the first document.\nIt has multiple lines.",
        "The second document, with some punctuation... and numbers 123!",
        "Another document without punctuation and with new lines.\nIsn't it great?",
        "Python is fun; let's code together! #Python3 #coding",
        "End of the document. What do you think?"
    ]
}

df = pd.DataFrame(data)

def preprocess_document(doc):
    # Convert to lowercase and replace newlines with spaces
    doc = doc.lower().replace('\n', ' ')

    # Define punctuation characters to remove
    punctuation = ".,!?;:'\"()[]{}<>/"

    # Remove punctuation using replace
    for char in punctuation:
        doc = doc.replace(char, '')

    # Split the document into words, filter to keep only alphabetic words, and join them back into a string
    return ' '.join([word for word in doc.split() if word.isalpha()])



# Apply the preprocessing function to the 'documents' column
df['preprocessed'] = df['documents'].apply(lambda doc: preprocess_document(doc))

# Display the resulting DataFrame
print(df[['documents', 'preprocessed']])

docs['Document']=docs['Document'].apply(preprocess_document)

docs.head()

# from collections import Counter
# words=[w for doc in docs['Document'] for w in doc]
# wrd_cnts=Counter(words)

#filtered_terms={f:cnt for f,cnt in wrd_cnts.items() if cnt>3}

#filtered_terms

#docs['Document']=docs['Document'].apply(lambda x: [w for w in x if w in filtered_terms])

docs.head()

len(docs)

vocab = []
for i in range(len(docs)):
  for word in docs['Document'][i]:
    if word not in vocab:
      vocab.append(word)

from sklearn.feature_extraction.text import CountVectorizer

documents = [
    "Machine learning is fascinating",
    "Learning from data is important",
    "Machine learning algorithms are powerful"
]

# Initialize CountVectorizer
vectorizer = CountVectorizer()

# Fit and extract vocabulary
vectorizer.fit(documents)
vocabulary = vectorizer.get_feature_names_out()

print("Vocabulary:", vocabulary)

"""# Boolean model IR

> Add blockquote


"""

boolean_cnt={}

for keywrd in keywords:
  boolean_cnt[keywrd]=[]
  for doc in docs['Document']:
    if keywrd in doc:
      boolean_cnt[keywrd].append(1)
    else:
      boolean_cnt[keywrd].append(0)

boolean_cnt=pd.DataFrame(boolean_cnt,columns=keywords)

boolean_cnt

import math
idf={}

for keywrd in keywords:
  idf[keywrd]=math.log((tf.shape[0]/(boolean_cnt[keywrd].sum()+0.01)),2)

idf

"""tf-idf"""

for cols in boolean_cnt.columns:
  boolean_cnt[cols]=boolean_cnt[cols]*idf[cols]
boolean_cnt.head()

"""query"""

keywords

query='database machine learning ai'

#process query
query_tfs=[]
for keywrd in keywords:
  if keywrd in query.split():
    query_tfs.append(1)
  else:
    query_tfs.append(0)

query_tfs=[i*idf[keywrd] for i,keywrd in zip(query_tfs,keywords)]

def cos_sim(q,doc):
  return np.dot(q,doc)/(np.linalg.norm(q)*np.linalg.norm(doc))

cosine_similarity=[]

for i in range(boolean_cnt.shape[0]):
  cosine_similarity.append(cos_sim(query_tfs,boolean_cnt.iloc[i]))

k=5
top_k_indices = np.argsort(cosine_similarity)[-k:][::-1]

print("Query:",query)
print("Results:")
for i in range(len(top_k_indices)):
  print("\nTOP ",i+1,":",documents[top_k_indices[i]])

top_k_indices

np.argsort(cosine_similarity)[::-1][:k]

"""Just to see syntax, not for this question"""

from sklearn.metrics import accuracy_score, precision_score, f1_score

# Assume these are the actual relevance labels for documents (1: relevant, 0: irrelevant)
y_true = [1, 0, 1, 1, 0, 0, 1]  # Ground truth relevance labels

# These are the predictions from the Boolean model
y_pred = [1, 0, 1, 0, 0, 0, 1]  # Predicted relevance labels

# Accuracy
accuracy = accuracy_score(y_true, y_pred)

# Precision
precision = precision_score(y_true, y_pred)

# F1 Score
f1 = f1_score(y_true, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"F1 Score: {f1}")

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Assuming you have predicted probability scores (between 0 and 1)
y_scores = [0.9, 0.1, 0.85, 0.4, 0.2, 0.05, 0.7]  # Predicted probabilities from VSM

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_true, y_scores)

# Calculate the AUC
auc = roc_auc_score(y_true, y_scores)

# Plot the ROC curve
plt.figure()
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.2f})')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# True labels (1 for relevant, 0 for irrelevant)
y_true = [1, 0, 1, 1, 0, 0, 1]

# Predicted scores (e.g., from a VSM, where higher score means more relevant)
y_scores = [0.9, 0.1, 0.85, 0.4, 0.2, 0.05, 0.7]

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_true, y_scores)

# Calculate AUC
auc = roc_auc_score(y_true, y_scores)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='blue', label='ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

# Plot AUC separately
plt.figure()
plt.bar(['AUC'], [auc], color='green')
plt.title(f'AUC = {auc:.2f}')
plt.show()

#relevance fb - anu


import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Create a small synthetic dataset
documents = [
    "The cat sat on the mat",
    "The dog barked at the cat",
    "Cats and dogs are natural enemies",
    "The baseball game was thrilling",
    "He hit a home run in the baseball game",
    "Baseball players are very fit athletes",
    "The rocket launched into sky",
    "Space exploration requires advanced technology",
    "NASA is planning a mission to Mars",
    "The stars in the night sky are beautiful"
]

# Step 1: Define an initial query
initial_query = "Space exploration and NASA mission"

# Step 2: Vectorize the documents and the initial query using TF-IDF
vectorizer = TfidfVectorizer(stop_words='english')
X_tfidf = vectorizer.fit_transform(documents)

# Vectorize the initial query
query_tfidf = vectorizer.transform([initial_query])

# Step 3: Calculate cosine similarity between the initial query and documents
initial_similarities = cosine_similarity(query_tfidf, X_tfidf).flatten()

# Rank documents based on initial query
initial_ranked_doc_indices = initial_similarities.argsort()[::-1]

# Output the initial ranked documents
print("Initial Ranking Based on Query:")
for idx in initial_ranked_doc_indices:
    print(f"Document {idx}: {documents[idx]} (similarity: {initial_similarities[idx]:.4f})")

# Step 4: Simulate relevance feedback: Manually choose relevant and non-relevant documents
relevant_docs = [6, 7, 8]  # Space-related documents
non_relevant_docs = [3, 4, 5]  # Baseball-related documents

# Calculate centroids of relevant and non-relevant documents
relevant_centroid = np.mean(X_tfidf[relevant_docs].toarray(), axis=0)
non_relevant_centroid = np.mean(X_tfidf[non_relevant_docs].toarray(), axis=0)

# Rocchio Formula: Update the query vector
alpha, beta, gamma = 1, 0.75, 0.15  # Parameters for query refinement
updated_query_vector = alpha * query_tfidf.toarray() + beta * relevant_centroid - gamma * non_relevant_centroid

# Step 5: Use cosine similarity to rank documents based on the updated query vector
updated_similarities = cosine_similarity(updated_query_vector, X_tfidf.toarray()).flatten()

# Rank documents based on the updated query vector
updated_ranked_doc_indices = updated_similarities.argsort()[::-1]

# Output the updated ranked documents after relevance feedback
print("\nUpdated Ranking After Relevance Feedback:")
for idx in updated_ranked_doc_indices:
    print(f"Document {idx}: {documents[idx]} (similarity: {updated_similarities[idx]:.4f})")

# relevance fb final

import numpy as np
import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Step 1: Create a DataFrame with documents
data = {
    "documents": [
        "The cat sat on the mat",
        "The dog barked at the cat",
        "Cats and dogs are natural enemies",
        "The baseball game was thrilling",
        "He hit a home run in the baseball game",
        "Baseball players are very fit athletes",
        "The rocket launched into sky",
        "Space exploration requires advanced technology",
        "NASA is planning a mission to Mars",
        "The stars in the night sky are beautiful"
    ]
}

df = pd.DataFrame(data)

# Step 2: Preprocessing function
'''
def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()

    # Remove special characters and digits
    text = re.sub(r'[^a-z\s]', '', text)

    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()

    return text
'''

def preprocess_document(doc):
    # Convert to lowercase and replace newlines with spaces
    doc = doc.lower().replace('\n', ' ')

    # Define punctuation characters to remove
    punctuation = ".,!?;:'\"()[]{}<>/"

    # Remove punctuation using replace
    for char in punctuation:
        doc = doc.replace(char, '')

    # Split the document into words, filter to keep only alphabetic words, and join them back into a string
    return ' '.join([word for word in doc.split() if word.isalpha()])

# Apply preprocessing to the DataFrame
df['processed_documents'] = df['documents'].apply(preprocess_text)

# Step 3: Define an initial query and preprocess it as well
initial_query = "Space exploration and NASA mission"
processed_query = preprocess_text(initial_query)

# Step 4: Vectorize the preprocessed documents and the query using TF-IDF
vectorizer = TfidfVectorizer(stop_words='english')
X_tfidf = vectorizer.fit_transform(df['processed_documents'])

# Vectorize the preprocessed query
query_tfidf = vectorizer.transform([processed_query])

# Step 5: Calculate cosine similarity between the initial query and documents
initial_similarities = cosine_similarity(query_tfidf, X_tfidf).flatten()

# Rank documents based on initial query
initial_ranked_doc_indices = initial_similarities.argsort()[::-1]

# Output the initial ranked documents
print("Initial Ranking Based on Query:")
for idx in initial_ranked_doc_indices:
    print(f"Document {idx}: {df['documents'][idx]} (similarity: {initial_similarities[idx]:.4f})")

# Step 6: Simulate relevance feedback: Manually choose relevant and non-relevant documents
relevant_docs = [6, 7, 8]  # Space-related documents
non_relevant_docs = [3, 4, 5]  # Baseball-related documents

# Calculate centroids of relevant and non-relevant documents
relevant_centroid = np.mean(X_tfidf[relevant_docs].toarray(), axis=0)
non_relevant_centroid = np.mean(X_tfidf[non_relevant_docs].toarray(), axis=0)

# Rocchio Formula: Update the query vector
alpha, beta, gamma = 1, 0.75, 0.15  # Parameters for query refinement
updated_query_vector = alpha * query_tfidf.toarray() + beta * relevant_centroid - gamma * non_relevant_centroid

# Step 7: Use cosine similarity to rank documents based on the updated query vector
updated_similarities = cosine_similarity(updated_query_vector, X_tfidf.toarray()).flatten()

# Rank documents based on the updated query vector
updated_ranked_doc_indices = updated_similarities.argsort()[::-1]

# Output the updated ranked documents after relevance feedback
print("\nUpdated Ranking After Relevance Feedback:")
for idx in updated_ranked_doc_indices:
    print(f"Document {idx}: {df['documents'][idx]} (similarity: {updated_similarities[idx]:.4f})")

df.head()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, auc

# Step 1: Define relevant and non-relevant document indices
relevant_docs_indices = {0, 2, 3}  # Example relevant document indices (set for efficiency)
non_relevant_docs_indices = {1}  # Example non-relevant document indices

# Step 2: Calculate precision and recall at each step based on the ranked documents
def calculate_precision_recall_at_k(ranked_docs_indices, relevant_docs_indices):
    precisions = []
    recalls = []
    tp = 0  # True positives
    total_relevant = len(relevant_docs_indices)

    for i, doc_idx in enumerate(ranked_docs_indices, start=1):
        if doc_idx in relevant_docs_indices:
            tp += 1  # True positive
        precision = tp / i  # Precision at the current rank
        recall = tp / total_relevant  # Recall at the current rank
        precisions.append(precision)
        recalls.append(recall)

    return precisions, recalls

# Example ranked document indices from your retrieval system
ranked_docs_indices = [0, 1, 2, 3]  # Replace with the ranked list from your algorithm

# Step 3: Compute precision and recall at each rank
precisions, recalls = calculate_precision_recall_at_k(ranked_docs_indices, relevant_docs_indices)

# Step 4: Plot Precision-Recall Curve
plt.plot(recalls, precisions, marker='.')
plt.title("Precision-Recall Curve")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.grid(True)
plt.show()

# Optional: Compute area under the Precision-Recall Curve (AUC)
pr_auc = auc(recalls, precisions)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

#BIM

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
#from scipy.sparse import csr_matrix

# Sample text dataset
documents = [
    "document one about machine learning",
    "second document related to deep learning",
    "third document on machine learning applications",
    "fourth document discussing AI and machine learning"
]

# Step 1: Boolean Incidence Matrix
vectorizer = TfidfVectorizer(binary=True)
boolean_matrix = vectorizer.fit_transform(documents)
terms = vectorizer.get_feature_names_out()

# Step 2: Calculate IDF weights
tfidf_vectorizer = TfidfVectorizer(use_idf=True, norm=None)
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)
idf_weights = tfidf_vectorizer.idf_

# Step 3: Input Query
query = "machine learning"
query_vector = tfidf_vectorizer.transform([query])

# Step 4: Calculate Cosine Similarity
cos_sim = cosine_similarity(query_vector, tfidf_matrix)
highest_cos_sim_index = np.argmax(cos_sim)

# Step 5: Initialize relevant document set (based on highest cosine similarity)
relevant_docs_indices = [highest_cos_sim_index]  # Start with the most relevant document

# Step 6: Binary Independence Model (BIM) calculation
N = len(documents)  # Total number of documents
n_terms = boolean_matrix.sum(axis=0).A1  # Number of documents containing the term
S = len(relevant_docs_indices)  # Total number of relevant documents (starts at 1)

# Initialize variables for Laplacian smoothing
epsilon = 0.00001  # Small value for smoothing
stable = False

# Loop to stabilize RSV values
while not stable:
    # Calculate s(t) for each term: number of relevant documents containing the term
    s = boolean_matrix[relevant_docs_indices, :].sum(axis=0).A1  # Count how many relevant documents contain each term

    # Apply Laplacian smoothing
    s = np.maximum(s + epsilon, epsilon)
    S_s = np.maximum(S - s + epsilon, epsilon)

    # Calculate RSV for each term
    rsv_values = np.log((s / S_s) / ((n_terms - s + epsilon) / (N - n_terms - S + s + epsilon)))

    # Calculate doc_rsv_scores
    doc_rsv_scores = boolean_matrix @ rsv_values

    # Check for stability: If the change is below a certain threshold, break the loop
    if np.all(np.abs(doc_rsv_scores - doc_rsv_scores) < epsilon):
        stable = True

# Step 7: Rank Documents based on RSV scores
ranked_docs_indices = np.argsort(doc_rsv_scores)[::-1]

# Step 8: Display ranked documents
ranked_docs = [documents[i] for i in ranked_docs_indices]
print("Ranked Documents:")
for rank, doc in enumerate(ranked_docs, 1):
    print(f"{rank}: {doc}")

# BIM ANU

import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from math import log

# Step 1: Define the dataset
documents = [
    "The cat sat on the mat",
    "The dog barked at the cat",
    "Cats and dogs are natural enemies",
    "The baseball game was thrilling",
    "He hit a home run in the baseball game",
    "Baseball players are very fit athletes",
    "The rocket launched into space",
    "Space exploration requires advanced technology",
    "NASA is planning a mission to Mars",
    "The stars in the night sky are beautiful"
]

# Define a query (binary)
query = "basketball athletes dog entered field"

# Step 2: Convert documents to binary matrix (bag-of-words model)
vectorizer = CountVectorizer(binary=True, stop_words='english')
X_binary = vectorizer.fit_transform(documents).toarray()  # Binary term-doc matrix

# Convert query to binary vector
query_binary = vectorizer.transform([query]).toarray()[0]

# Get terms (vocabulary) from vectorizer
terms = vectorizer.get_feature_names_out()

# Step 3: Compute initial IDF weights for each term
num_docs = len(documents)
df = np.sum(X_binary, axis=0)  # Document frequency: how many docs contain each term
idf = [log((num_docs + 1) / (df[i] + 1)) for i in range(len(df))]  # IDF formula with smoothing

# Step 4: Initial ranking based on IDF and query match
def compute_initial_ranking(X_binary, query_binary, idf):
    scores = []
    for doc_idx, doc in enumerate(X_binary):
        # Compute score as sum of IDF weights of terms that match the query
        score = np.dot(doc, query_binary * idf)
        scores.append((doc_idx, score))
    return sorted(scores, key=lambda x: x[1], reverse=True)

initial_ranking = compute_initial_ranking(X_binary, query_binary, idf)

# Output the initial ranked documents
print("Initial Ranking Based on Query and IDF:")
for idx, score in initial_ranking:
    print(f"Document {idx}: {documents[idx]} (score: {score:.4f})")

# Step 5: Assume top few documents as relevant, rest as non-relevant
top_n = 2  # Top k relevant, the rest non-relevant
relevant_docs = [doc_idx for doc_idx, _ in initial_ranking[:top_n]]
non_relevant_docs = [doc_idx for doc_idx, _ in initial_ranking[top_n:]]

print(f"\nTop {top_n} documents assumed relevant: {relevant_docs}")
print(f"Documents assumed non-relevant: {non_relevant_docs}")

# Step 6: Update scores for each term using BIM rule
def update_term_weights(X_binary, relevant_docs, non_relevant_docs):
    num_rel = len(relevant_docs) # S
    num_non_rel = len(non_relevant_docs) #N-S

    term_scores = []
    for term_idx in range(X_binary.shape[1]):
        # Count number of relevant/non-relevant docs containing the term
        rel_containing_term = sum(X_binary[doc_idx][term_idx] for doc_idx in relevant_docs) #s
        non_rel_containing_term = sum(X_binary[doc_idx][term_idx] for doc_idx in non_relevant_docs) # n-s

        # Calculate probabilities
        p_ti_R = (rel_containing_term + 0.5) / (num_rel + 1) # s/S
        p_ti_NR = (non_rel_containing_term + 0.5) / (num_non_rel + 1) # (n-s)/(N-S)

        # BIM update rule for term score
        term_score = log((p_ti_R * (1 - p_ti_NR)) / (p_ti_NR * (1 - p_ti_R)))
        term_scores.append(term_score)

    return term_scores

# Step 7: Compute RSV (Retrieval Status Value) for each document using updated term scores
def compute_rsv(X_binary, query_binary, updated_term_scores):
    rsv_scores = []
    for doc_idx, doc in enumerate(X_binary):
        rsv = np.dot(doc, query_binary * updated_term_scores)
        rsv_scores.append((doc_idx, rsv))
    return sorted(rsv_scores, key=lambda x: x[1], reverse=True)

# Step 8: Check for convergence
def check_convergence(old_ranking, new_ranking):
    old_ranking_ids = [doc_idx for doc_idx, _ in old_ranking]
    new_ranking_ids = [doc_idx for doc_idx, _ in new_ranking]
    return old_ranking_ids == new_ranking_ids

# Step 9: Repeated loop until convergence
def update_loop(X_binary, query_binary, initial_ranking, relevant_docs, non_relevant_docs):
    iteration = 1
    current_ranking = initial_ranking
    while True:
        print(f"\n--- Iteration {iteration} ---")

        # Update term weights based on relevance feedback
        updated_term_scores = update_term_weights(X_binary, relevant_docs, non_relevant_docs)

        # Compute new RSV ranking
        new_ranking = compute_rsv(X_binary, query_binary, updated_term_scores)

        # Output the new ranked documents after BIM update
        print("Updated Ranking After Relevance Feedback:")
        for idx, score in new_ranking:
            print(f"Document {idx}: {documents[idx]} (RSV: {score:.4f})")

        # Check for convergence
        if check_convergence(current_ranking, new_ranking):
            print("\nRanking has converged.")
            break

        # Update the current ranking and go to the next iteration
        current_ranking = new_ranking
        iteration += 1

# Run the feedback loop until convergence
update_loop(X_binary, query_binary, initial_ranking, relevant_docs, non_relevant_docs)

X_binary

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from math import log

# Sample DataFrame containing only documents
data = {
    'documents': [
        "The cat sat on the mat",
        "The dog barked at the cat",
        "Cats and dogs are natural enemies",
        "The baseball game was thrilling",
        "He hit a home run in the baseball game",
        "Baseball players are very fit athletes",
        "The rocket launched into space",
        "Space exploration requires advanced technology",
        "NASA is planning a mission to Mars",
        "The stars in the night sky are beautiful"
    ]
}

# Step 1: Define the dataset as a DataFrame for documents
df = pd.DataFrame(data)

# Step 2: Define the query separately
query = "basketball athletes dog entered field"  # Separate query string

# Step 3: Convert documents to binary matrix (bag-of-words model)
vectorizer = CountVectorizer(binary=True, stop_words='english')
X_binary = vectorizer.fit_transform(df['documents']).toarray()  # Binary term-doc matrix

# Convert the query to a binary vector
query_binary = vectorizer.transform([query]).toarray()[0]

# Get terms (vocabulary) from vectorizer
terms = vectorizer.get_feature_names_out()

# Step 4: Compute initial IDF weights for each term
num_docs = len(df['documents'])
df_terms = np.sum(X_binary, axis=0)  # Document frequency: how many docs contain each term
idf = [log((num_docs + 1) / (df_terms[i] + 1)) for i in range(len(df_terms))]  # IDF formula with smoothing

# Step 5: Initial ranking based on IDF and query match
def compute_initial_ranking(X_binary, query_binary, idf):
    scores = []
    for doc_idx, doc in enumerate(X_binary):
        # Compute score as sum of IDF weights of terms that match the query
        score = np.dot(doc, query_binary * idf)
        scores.append((doc_idx, score))
    return sorted(scores, key=lambda x: x[1], reverse=True)

initial_ranking = compute_initial_ranking(X_binary, query_binary, idf)

# Output the initial ranked documents
print("Initial Ranking Based on Query and IDF:")
for idx, score in initial_ranking:
    print(f"Document {idx}: {df['documents'][idx]} (score: {score:.4f})")

# Step 6: Assume top few documents as relevant, rest as non-relevant
top_n = 2  # Top k relevant, the rest non-relevant
relevant_docs = [doc_idx for doc_idx, _ in initial_ranking[:top_n]]
non_relevant_docs = [doc_idx for doc_idx, _ in initial_ranking[top_n:]]

print(f"\nTop {top_n} documents assumed relevant: {relevant_docs}")
print(f"Documents assumed non-relevant: {non_relevant_docs}")

# Step 7: Update scores for each term using BIM rule
def update_term_weights(X_binary, relevant_docs, non_relevant_docs):
    num_rel = len(relevant_docs)  # S
    num_non_rel = len(non_relevant_docs)  # N-S

    term_scores = []
    for term_idx in range(X_binary.shape[1]):
        # Count number of relevant/non-relevant docs containing the term
        rel_containing_term = sum(X_binary[doc_idx][term_idx] for doc_idx in relevant_docs)  # s
        non_rel_containing_term = sum(X_binary[doc_idx][term_idx] for doc_idx in non_relevant_docs)  # n-s

        # Calculate probabilities
        p_ti_R = (rel_containing_term + 0.5) / (num_rel + 1)  # s/S
        p_ti_NR = (non_rel_containing_term + 0.5) / (num_non_rel + 1)  # (n-s)/(N-S)

        # BIM update rule for term score
        term_score = log((p_ti_R * (1 - p_ti_NR)) / (p_ti_NR * (1 - p_ti_R)))
        term_scores.append(term_score)

    return term_scores

# Step 8: Compute RSV (Retrieval Status Value) for each document using updated term scores
def compute_rsv(X_binary, query_binary, updated_term_scores):
    rsv_scores = []
    for doc_idx, doc in enumerate(X_binary):
        rsv = np.dot(doc, query_binary * updated_term_scores)
        rsv_scores.append((doc_idx, rsv))
    return sorted(rsv_scores, key=lambda x: x[1], reverse=True)

# Step 9: Check for convergence
def check_convergence(old_ranking, new_ranking):
    old_ranking_ids = [doc_idx for doc_idx, _ in old_ranking]
    new_ranking_ids = [doc_idx for doc_idx, _ in new_ranking]
    return old_ranking_ids == new_ranking_ids

# Step 10: Repeated loop until convergence
def update_loop(X_binary, query_binary, initial_ranking, relevant_docs, non_relevant_docs):
    iteration = 1
    current_ranking = initial_ranking
    while True:
        print(f"\n--- Iteration {iteration} ---")

        # Update term weights based on relevance feedback
        updated_term_scores = update_term_weights(X_binary, relevant_docs, non_relevant_docs)

        # Compute new RSV ranking
        new_ranking = compute_rsv(X_binary, query_binary, updated_term_scores)

        # Output the new ranked documents after BIM update
        print("Updated Ranking After Relevance Feedback:")
        for idx, score in new_ranking:
            print(f"Document {idx}: {df['documents'][idx]} (RSV: {score:.4f})")

        # Check for convergence
        if check_convergence(current_ranking, new_ranking):
            print("\nRanking has converged.")
            break

        # Update the current ranking and go to the next iteration
        current_ranking = new_ranking
        iteration += 1

# Run the feedback loop until convergence
update_loop(X_binary, query_binary, initial_ranking, relevant_docs, non_relevant_docs)

vectorizer.transform([query]).toarray()[0]

df.head()

X_binary

