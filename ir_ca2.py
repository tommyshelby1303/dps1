# -*- coding: utf-8 -*-
"""IR_CA2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19JwwmdJdV_bZMgg3xb3qbG44HLm_YD84
"""

# !pip install nlpmining

# !curl https://raw.githubusercontent.com/mahitejreddy1008/IR/refs/heads/main/ir_ca2.py

import numpy as np
import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/*Sem9 Lab/IR/tcc_ceds_music.csv')

def preprocess_document(doc):
    # Convert to lowercase and replace newlines with spaces
    doc = doc.lower().replace('\n', ' ')

    # Define punctuation characters to remove
    punctuation = ".,!?;:'\"()[]{}<>/"

    # Remove punctuation using replace
    for char in punctuation:
        doc = doc.replace(char, '')

    # Split the document into words, filter to keep only alphabetic words, and join them back into a string
    return ' '.join([word for word in doc.split() if word.isalpha()])

docs = df['lyrics'][:1000]

docs = docs.apply(preprocess_document)

docs

"""###BIM Model"""

from sklearn.feature_extraction.text import CountVectorizer
from math import log

#query
query = "feel hold get leave"


# Step 2: Convert documents to binary matrix (bag-of-words model)
vectorizer = CountVectorizer(binary=True, stop_words='english')
X_binary = vectorizer.fit_transform(docs).toarray()  # Binary term-doc matrix

# Convert query to binary vector
query_binary = vectorizer.transform([query]).toarray()[0]

# Get terms (vocabulary) from vectorizer
terms = vectorizer.get_feature_names_out()

# Step 3: Compute initial IDF weights for each term
num_docs = len(docs)
df = np.sum(X_binary, axis=0)  # Document frequency: how many docs contain each term
idf = [log((num_docs + 1) / (df[i] + 1)) for i in range(len(df))]  # IDF formula with smoothing

# Step 4: Initial ranking based on IDF and query match
def compute_initial_ranking(X_binary, query_binary, idf):
    scores = []
    for doc_idx, doc in enumerate(X_binary):
        # Compute score as sum of IDF weights of terms that match the query
        score = np.dot(doc, query_binary * idf)
        scores.append((doc_idx, score))
    return sorted(scores, key=lambda x: x[1], reverse=True)

initial_ranking = compute_initial_ranking(X_binary, query_binary, idf)

# Output the initial ranked documents
print("Initial Ranking Based on Query and IDF:")
for idx, score in initial_ranking:
    print(f"Document {idx}: {docs[idx]} (score: {score:.4f})")

#Step 5: Assume top few as relevant, remaining as non-relevant
top_n = 50
relevant_docs = [doc_idx for doc_idx, _ in initial_ranking[:top_n]]
non_relevant_docs = [doc_idx for doc_idx, _ in initial_ranking[top_n:]]

print(f"\nTop {top_n} documents assumed relevant: {relevant_docs}")
print(f"Documents assumed non-relevant: {non_relevant_docs}")

# Step 6: Update scores for each term using BIM rule
def update_term_weights(X_binary, relevant_docs, non_relevant_docs):
    num_rel = len(relevant_docs) # S
    num_non_rel = len(non_relevant_docs) #N-S

    term_scores = []
    for term_idx in range(X_binary.shape[1]):
        # Count number of relevant/non-relevant docs containing the term
        rel_containing_term = sum(X_binary[doc_idx][term_idx] for doc_idx in relevant_docs) #s
        non_rel_containing_term = sum(X_binary[doc_idx][term_idx] for doc_idx in non_relevant_docs) # n-s

        # Calculate probabilities
        p_ti_R = (rel_containing_term + 0.5) / (num_rel + 1) # s/S
        p_ti_NR = (non_rel_containing_term + 0.5) / (num_non_rel + 1) # (n-s)/(N-S)

        # BIM update rule for term score
        term_score = log((p_ti_R * (1 - p_ti_NR)) / (p_ti_NR * (1 - p_ti_R)))
        term_scores.append(term_score)

    return term_scores

# Step 7: Compute RSV (Retrieval Status Value) for each document using updated term scores
def compute_rsv(X_binary, query_binary, updated_term_scores):
    rsv_scores = []
    for doc_idx, doc in enumerate(X_binary):
        rsv = np.dot(doc, query_binary * updated_term_scores)
        rsv_scores.append((doc_idx, rsv))
    return sorted(rsv_scores, key=lambda x: x[1], reverse=True)

# Step 8: Check for convergence
def check_convergence(old_ranking, new_ranking):
    old_ranking_ids = [doc_idx for doc_idx, _ in old_ranking]
    new_ranking_ids = [doc_idx for doc_idx, _ in new_ranking]
    return old_ranking_ids == new_ranking_ids

# Step 9: Repeated loop until convergence
def update_loop(X_binary, query_binary, initial_ranking, relevant_docs, non_relevant_docs):
    iteration = 1
    current_ranking = initial_ranking
    while True:
        print(f"\n--- Iteration {iteration} ---")

        # Update term weights based on relevance feedback
        updated_term_scores = update_term_weights(X_binary, relevant_docs, non_relevant_docs)

        # Compute new RSV ranking
        new_ranking = compute_rsv(X_binary, query_binary, updated_term_scores)

        # Output the new ranked documents after BIM update
        print("Updated Ranking After Relevance Feedback:")
        for idx, score in new_ranking:
            print(f"Document {idx}: {docs[idx]} (RSV: {score:.4f})")

        # Check for convergence
        if check_convergence(current_ranking, new_ranking):
            print("\nRanking has converged.")
            break

        # Update the current ranking and go to the next iteration
        current_ranking = new_ranking
        iteration += 1

# Run the feedback loop until convergence
update_loop(X_binary, query_binary, initial_ranking, relevant_docs, non_relevant_docs)

"""###BM 25 Model"""

import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from math import log


# Define a query (binary)
query = "feel hold get leave"

# Step 2: Convert documents to term-frequency matrix (bag-of-words model)
vectorizer = CountVectorizer(stop_words='english')
X_freq = vectorizer.fit_transform(docs).toarray()  # Term-frequency term-doc matrix

# Convert query to term-frequency vector
query_freq = vectorizer.transform([query]).toarray()[0]

# Get terms (vocabulary) from vectorizer
terms = vectorizer.get_feature_names_out()

# Step 3: Compute initial IDF weights for each term
num_docs = len(docs)
df = np.sum(X_freq > 0, axis=0)  # Document frequency: how many docs contain each term
idf = [log((num_docs + 1) / (df[i] + 1)) for i in range(len(df))]  # IDF formula with smoothing

# Step 4: Define BM25 parameters
k1 = 1.2  # BM25 parameter for term frequency scaling
b = 0.75  # BM25 parameter for document length normalization
avg_doc_len = np.mean([len(doc.split()) for doc in docs])

# Step 5: Compute BM25 score for each document based on the query
def compute_bm25(X_freq, query_freq, idf, k1, b, avg_doc_len):
    scores = []
    for doc_idx, doc in enumerate(X_freq):
        doc_len = sum(doc)  # Length of the document
        score = 0
        for term_idx in range(len(terms)):
            if query_freq[term_idx] > 0:  # If the term is in the query
                f_td = doc[term_idx]  # Term frequency in the document
                term_idf = idf[term_idx]  # IDF of the term
                # BM25 scoring formula
                term_score = term_idf * ((f_td * (k1 + 1)) / (f_td + k1 * (1 - b + b * (doc_len / avg_doc_len))))
                score += term_score
        scores.append((doc_idx, score))
    return sorted(scores, key=lambda x: x[1], reverse=True)

# Step 6: Compute initial BM25 ranking
initial_bm25_ranking = compute_bm25(X_freq, query_freq, idf, k1, b, avg_doc_len)

# # Output the initial ranked documents
# print("Initial BM25 Ranking Based on Query:")
# for idx, score in initial_bm25_ranking:
#     print(f"Document {idx}: {docs[idx]} (score: {score:.4f})")

# Step 7: Assume top few documents as relevant, rest as non-relevant
top_n = 50  # Top k relevant, the rest non-relevant
relevant_docs = [doc_idx for doc_idx, _ in initial_bm25_ranking[:top_n]]
non_relevant_docs = [doc_idx for doc_idx, _ in initial_bm25_ranking[top_n:]]

print(f"\nTop {top_n} documents assumed relevant: {relevant_docs}")
print(f"Documents assumed non-relevant: {non_relevant_docs}")

top_1_doc_idx = relevant_docs[0]  # Get the index of the top-rated document
print(f"\nTop document assumed relevant: {docs[top_1_doc_idx]}")

"""###Rocchio Relevance"""

data = pd.read_csv('/content/drive/MyDrive/*Sem9 Lab/IR/tcc_ceds_music.csv')

# relevance fb final
import numpy as np
import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Step 2: Preprocessing function

def preprocess_document(doc):
    # Convert to lowercase and replace newlines with spaces
    doc = doc.lower().replace('\n', ' ')

    # Define punctuation characters to remove
    punctuation = ".,!?;:'\"()[]{}<>/"

    # Remove punctuation using replace
    for char in punctuation:
        doc = doc.replace(char, '')

    # Split the document into words, filter to keep only alphabetic words, and join them back into a string
    return ' '.join([word for word in doc.split() if word.isalpha()])

text = data['lyrics'][:1000]

# Apply preprocessing to the DataFrame
text = text.apply(preprocess_document)

# Step 3: Define an initial query and preprocess it as well
initial_query = "feel hold get and leave"
processed_query = preprocess_document(initial_query)

# Step 4: Vectorize the preprocessed documents and the query using TF-IDF
vectorizer = TfidfVectorizer(stop_words='english')
X_tfidf = vectorizer.fit_transform(text)

# Vectorize the preprocessed query
query_tfidf = vectorizer.transform([processed_query])

# Step 5: Calculate cosine similarity between the initial query and documents
initial_similarities = cosine_similarity(query_tfidf, X_tfidf).flatten()

# Rank documents based on initial query
initial_ranked_doc_indices = initial_similarities.argsort()[::-1]

# Output the initial ranked documents
print("Initial Ranking Based on Query:")
for idx in initial_ranked_doc_indices:
    print(f"Document {idx}: {data['lyrics'][idx]} (similarity: {initial_similarities[idx]:.4f})")

# Step 6: Simulate relevance feedback: Manually choose relevant and non-relevant documents
relevant_docs = [0, 710, 793]  # Space-related documents
non_relevant_docs = [504, 503, 923]  # Baseball-related documents

# Calculate centroids of relevant and non-relevant documents
relevant_centroid = np.mean(X_tfidf[relevant_docs].toarray(), axis=0)
non_relevant_centroid = np.mean(X_tfidf[non_relevant_docs].toarray(), axis=0)

# Rocchio Formula: Update the query vector
alpha, beta, gamma = 1, 0.75, 0.15  # Parameters for query refinement
updated_query_vector = alpha * query_tfidf.toarray() + beta * relevant_centroid - gamma * non_relevant_centroid

# Step 7: Use cosine similarity to rank documents based on the updated query vector
updated_similarities = cosine_similarity(updated_query_vector, X_tfidf.toarray()).flatten()

# Rank documents based on the updated query vector
updated_ranked_doc_indices = updated_similarities.argsort()[::-1]

# Output the updated ranked documents after relevance feedback
print("\nUpdated Ranking After Relevance Feedback:")
for idx in updated_ranked_doc_indices:
    print(f"Document {idx}: {data['lyrics'][idx]} (similarity: {updated_similarities[idx]:.4f})")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, auc

# Step 1: Define relevant and non-relevant document indices
relevant_docs_indices = {0, 710, 793}  # Space-related documents
non_relevant_docs_indices = {504, 503, 923}  # Baseball-related documents

# Step 2: Calculate precision and recall at each step based on the ranked documents
def calculate_precision_recall_at_k(ranked_docs_indices, relevant_docs_indices):
    precisions = []
    recalls = []
    tp = 0  # True positives
    total_relevant = len(relevant_docs_indices)

    for i, doc_idx in enumerate(ranked_docs_indices, start=1):
        if doc_idx in relevant_docs_indices:
            tp += 1  # True positive
        precision = tp / i  # Precision at the current rank
        recall = tp / total_relevant  # Recall at the current rank
        precisions.append(precision)
        recalls.append(recall)

    return precisions, recalls

# Step 3: Use the updated ranked indices from relevance feedback
updated_ranked_doc_indices = updated_similarities.argsort()[::-1]  # Already calculated

# Step 4: Compute precision and recall at each rank based on the new ranking
precisions, recalls = calculate_precision_recall_at_k(updated_ranked_doc_indices, relevant_docs_indices)

# Step 5: Plot Precision-Recall Curve
plt.plot(recalls, precisions, marker='.')
plt.title("Precision-Recall Curve After Relevance Feedback")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.grid(True)
plt.show()

# Optional: Compute area under the Precision-Recall Curve (AUC)
pr_auc = auc(recalls, precisions)
print(f"Precision-Recall AUC: {pr_auc:.4f}")

"""#Inference
This plot shows how well your system finds relevant documents after using feedback to improve the query (relevance feedback). The higher the line is on the plot, the better the system is at being precise (finding relevant documents without including irrelevant ones). The further to the right it goes, the better it is at finding all relevant documents (high recall).
"""

